<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Want to understand the average size of hidden representations? - Learning Mechanics</title>
  
  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script defer src="../static/math-render.js"></script>
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="../static/style.css">
  
  <!-- Meta tags -->
  <meta name="description" content="Exploring the mathematical properties of hidden layer representations and their average magnitudes.">
  <meta name="author" content="[long list of authors]">
  <meta name="date" content="2025-09-01">
</head>
<body id="top">
  <a href="../index.html" class="home-glyph"><i class="fas fa-home"></i></a>
  <button class="theme-toggle" onclick="toggleTheme()"><i class="fas fa-sun"></i></button>

  <article>
    <header>
            
            <h1>Want to understand the average size of hidden representations?</h1>
            <div class="sequence-nav">
        Part 2 of <a href="../quickstart/introduction.html">A Quickstart Guide to Learning Mechanics</a> (<a href="../quickstart/introduction.html">prev</a> | <a href="../quickstart/hyperparameter-selection.html">next</a>)
      </div>
            <hr class="title-separator">
      <div class="metadata">
        [long list of authors]
        <br><time datetime="2025-09-01">2025-09-01</time>
      </div>
      <hr class="metadata-separator">
          </header>

    <main>
      <p>You should! This is foundational for understanding everything else about the dynamics of neural networks.</p>
      <p>Historically, the first questions people tried to answer about neural networks dealt with their performance and representations: how can we characterize how well our network performs, and what hidden representations do they learn as they train? We‚Äôll revisit these questions later in a modern light, but suffice it to say that they are hard and it‚Äôs unclear where to start. In rigorous science, it‚Äôs usually a good idea to be humble about what you can understand and to start with the dumbest, simplest question you think you can answer, working up from there. It turns out that the simplest useful theoretical question you can ask about neural networks is: <em>as you forward-propagate your signal and backprop your gradient, roughly how big are the (pre)activations and gradients on average?</em></p>
      <p>More precisely: suppose we have an input vector <span class="math inline">\(\mathbf{x}\)</span> (an image, token, set of features, etc.), and we start propagating forward through our network, stopping part way. Denote by <span class="math inline">\(\mathbf{h}_\ell(\mathbf{x})\)</span> the hidden representations after applying the <span class="math inline">\(\ell\)</span>-th linear layer. What‚Äôs the typical size of an element of <span class="math inline">\(\mathbf{h}_\ell(\mathbf{x})\)</span>? If you want a mathematical metric for this, you might study the root-mean-squared size</p>
      <p><span class="math display">\[
      q_\ell(\mathbf{x}) := \frac{|\!|{\mathbf{h}_\ell(\mathbf{x})}|\!|}{\sqrt{\text{size}[\mathbf{h}_\ell(\mathbf{x})]}}.
      \]</span></p>
      <p>You don‚Äôt want <span class="math inline">\(q_\ell(\mathbf{x})\)</span> to either blow up or vanish as you propagate forwards through the network. If either happens, you‚Äôll be feeding very large or very small arguments to your activation function<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, which is generally a bad idea. In a neural network of many layers, problems like this tend to get worse as you propagate through more and more layers, so you want to avoid them from the get go.</p>
      <h3 id="first-steps-lecun-initialization-and-large-width">First steps: LeCun initialization and large width</h3>
      <p>The first people to address this question seriously were practitioners in the 1990s. If you initialize a neural network‚Äôs weight parameters with unit variance ‚Äî that is, <span class="math inline">\(W_{ij} \sim \mathcal{N}(0,1)\)</span> ‚Äî then your preactivations tend to blow up. If instead you initialize with</p>
      <p><span class="math display">\[
      W_{ij} \sim \mathcal{N}(0, \sigma^2),
      \]</span></p>
      <p>where <span class="math inline">\(\sigma^2 = \frac{1}{\text{[fan in]}}\)</span>. By a central limit theorem calculation, this ensures that well-behaved activations in the previous layer mean well-behaved preactivations in the following layer, at least at initialization.</p>
      <ul>
      <li>This scheme is now called LeCun initialization after <a href="https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3">[LeCun et al.¬†(1996)]</a>.</li>
      </ul>
      <p>This is the first calculation any new deep learning scientist should be able to perform! Get comfortable with this kind of central limit theorem argument.</p>
      <p>The central limit theorem works best as the number of added terms grows. This suggests that, when studying any sort of signal propagation problem, it‚Äôs a good and useful idea to consider the case of large width. <strong>This is the consideration that motivates the study of infinite-width networks, which are now central in the mathematical science of deep learning.</strong> It‚Äôs very important to think about this enough to have intuition for why the large-width limit is useful.</p>
      <h3 id="infinite-width-nets-at-init-signal-propagation-and-the-nngp">Infinite-width nets at init: signal propagation and the NNGP</h3>
      <p>A natural next question is: <em>what else can you say about wide neural networks at initialization?</em> The answer unfolds in the following sequence.</p>
      <p>First, you can perform a close study of the ‚Äúsignal sizes‚Äù <span class="math inline">\(q_\ell(\mathbf{x})\)</span> as well as the correlations <span class="math inline">\(c_\ell(\mathbf{x}, \mathbf{x}&#39;) := \langle \mathbf{h}_\ell(\mathbf{x}), \mathbf{h}_\ell(\mathbf{x}&#39;) \rangle\)</span>. You can actually calculate both of these exactly at infinite width using Gaussian integrals.</p>
      <ul>
      <li>See <a href="https://arxiv.org/abs/1606.05340">[Poole et al.¬†(2016)]</a> for a derivation physicists will like and <a href="https://arxiv.org/abs/1602.05897">[Daniely et al.¬†(2016)]</a> for a more formal treatment mathematicians will like.
      <ul>
      <li>It‚Äôs worth getting to the point where you understand either Eqns (1-5) from <a href="https://arxiv.org/abs/1606.05340">[Poole et al.¬†(2016)]</a> or Sections 5 and 8 from [Daniely et al (2016)]. The other stuff ‚Äî chaos, computational graphs, and so on ‚Äî is cool but not essential.</li>
      </ul></li>
      <li>See <a href="https://arxiv.org/abs/1611.01232">[Schoenholz et al.¬†(2016)]</a> for a similar size analysis for the backpropagated gradients.</li>
      </ul>
      <p>Next, you can study not only the <em>averages</em> of these quantities but also their complete <em>distributions</em>. It turns out they‚Äôre Gaussian at initialization (surprise, surprise) and the network function value itself is a ‚ÄúGaussian process‚Äù with a covariance kernel that you can obtain in closed form.</p>
      <ul>
      <li>This was first worked out for shallow networks by <a href="https://glizen.com/radfordneal/ftp/pin.pdf">[Neal (1996)]</a>. It took another two decades before it was extended to deep networks by <a href="https://arxiv.org/abs/1711.00165">[Lee et al.¬†(2017)]</a>.</li>
      </ul>
      <p>It‚Äôs very worth working through the neural network Gaussian process (NNGP) idea and getting intuition for both GPs and the forward-prop statistics that give a GP in this context. Notice that MLPs have NNGPs with rotation-invariant kernels.</p>
      <p>At this point in our discussion, we already have papers that have calculated average-case quantities exactly which agree well with experiments using networks with widths in the hundreds or thousands. Look at how good the agreement is in these plots:</p>
      <div class="full-width-figure">
      <img src="../static/great_dl_th-exp_plots.png" alt="Theory-experiment agreement plots">
      <div class="figure-caption">
      Left: signal propagation of layerwise correlations from <a href="https://arxiv.org/abs/1606.05340">Poole et al.¬†(2016)</a>. Right: performance vs.¬†ordered/chaotic regimes from <a href="https://arxiv.org/abs/1711.00165">Lee et al.¬†(2017)</a>. Now <em>that‚Äôs</em> a theory-experiment match.
      </div>
      </div>
      <p>It‚Äôs worth appreciating that extremely good agreement with experiment is possible if we‚Äôre studying the right objects in the right regimes. Most deep learning theory work that can‚Äôt get agreement this good eventually fades or is replaced by something that does. It‚Äôs usually wise to insist on a quantitative match from your theory and be satisfied with nothing less.</p>
      <h3 id="infinite-width-nets-under-gradient-descent-the-ntk">Infinite-width nets under gradient descent: the NTK</h3>
      <p>Now that we understand initialization in wide networks, we‚Äôre ready to study training. The first milestone on this path is the ‚Äúneural tangent kernel‚Äù (NTK). The main result here is that if you <em>train</em> a neural network in the NNGP infinite-width setting, its functional evolution is described by a particular kernel which remains static for all time. This kernel is the inner product of parameter gradient vectors:</p>
      <p><span class="math display">\[
      \text{NTK}(\mathbf{x}, \mathbf{x}&#39;) := \left\langle \nabla_{\boldsymbol{\theta}} f_{\boldsymbol{\theta}}(\mathbf{x}), \nabla_{\boldsymbol{\theta}} f_{\boldsymbol{\theta}}(\mathbf{x}&#39;) \right\rangle.
      \]</span></p>
      <p>The primary consequence is that, in this limit, the learning dynamics of a neural networks is, by the ‚Äúkernel trick,‚Äù equivalent to the dynamics of a linear model. The final learned function is given by <em>kernel (ridge) regression.</em></p>
      <ul>
      <li>The original NTK paper is that of <a href="https://arxiv.org/abs/1806.07572">[Jacot et al.¬†(2018)]</a>. The subsequent presentation of <a href="https://arxiv.org/abs/1902.06720">[Lee et al.¬†(2019)]</a> is less formal and may be easier for beginners.
      <ul>
      <li>The NTK idea is a bit too technical to explain here (though we sure want to), but it‚Äôs all but essential to understand it before moving on to feature learning. It‚Äôs worth allocating some time, working through one of these papers, and making sure you‚Äôve extracted the simple core idea. It‚Äôs also worth thinking carefully about linear models and kernel regression, as these will return later as first models for other learning phenomena.</li>
      <li>Notice that one can make accurate <em>quantitative</em> predictions for the learning of wide nets using the NTK. See, for example, Figure 2 of <a href="https://arxiv.org/abs/1902.06720">[Lee et al.¬†(2019)]</a>.</li>
      </ul></li>
      </ul>
      <h3 id="the-dynamics-of-feature-learning-the-maximal-update-parameterization-mup">The dynamics of feature learning: the maximal update parameterization (<span class="math inline">\(\mu\)</span>P)</h3>
      <p>After the development of the NTK, people quickly noticed that networks in this limit don‚Äôt exhibit feature learning. That is, at infinite width, the hidden neurons of a network represent the same functions after training as they did at initialization. At large-but-finite width, the change is finite but negligible. This is a first clue that the wonderful, analytically tractable NTK limit isn‚Äôt the end of the story.</p>
      <p>For a few years, it seemed like we might have to give up on infinite-width networks. Fortunately, it turned out that there‚Äôs <em>another</em> coherent infinite-width limit in which things scale differently, and the network <em>does</em> actually undergo feature learning. This is the limit in which most deep learning theory now takes place.</p>
      <p>Here‚Äôs the evolution of ideas, some key papers, and key takeaways:</p>
      <ul>
      <li><a href="https://arxiv.org/abs/1812.07956">[Chizat and Bach (2018)]</a> gave a ‚Äúlazy‚Äù vs.¬†‚Äúrich‚Äù dichotomy of gradient descent dynamics, using ‚Äúlazy‚Äù to mean ‚Äúkernel dynamics‚Äù and ‚Äúrich‚Äù to mean ‚Äúanything else.‚Äù
      <ul>
      <li>It‚Äôs worth understanding what ‚Äúlazy‚Äù training is, and how an output multiplier can make any neural network train lazily.</li>
      </ul></li>
      <li><a href="https://arxiv.org/abs/1804.06561">[Mei et al (2018)]</a> pointed out a ‚Äúmean-field‚Äù parameterization that allows infinite-width shallow neural nets to learn features.
      <ul>
      <li>This may be an easier place to start than <span class="math inline">\(\mu\)</span>P, but if you understand <span class="math inline">\(\mu\)</span>P, you can skip mean-field nets for now.</li>
      </ul></li>
      <li><a href="https://proceedings.mlr.press/v139/yang21c.html">[Yang and Hu (2021)]</a> pointed out a way that layerwise init sizes + learning rates can scale with network width so that the network learns features to leading order <em>even at infinite width.</em> They call this the ‚Äúmaximal update parameterization‚Äù (muP, or <span class="math inline">\(\mu\)</span>P).
      <ul>
      <li>The core idea here is extremely important and can be understood in simple terms. Their ‚ÄúTensor Programs‚Äù and ‚Äúabc-param‚Äù frameworks are fairly complicated ‚Äî you don‚Äôt need to understand either to get the main gist. <a href="https://arxiv.org/abs/2310.17813">[Yang et al.¬†(2023)]</a> gives a simpler framing of the big idea here.</li>
      </ul></li>
      <li><a href="https://arxiv.org/abs/2203.03466">[Yang et al.¬†(2022)]</a> showed that this parameterization is practically useful: it lets one scale up neural networks while preserving network hyperparameters. (More on this in <a href="LINK">our discussion of hyperparameters</a>.)
      <ul>
      <li>This is widely hailed as the first practically impactful achievement of deep learning theory.</li>
      </ul></li>
      </ul>
      <p>These ‚Äúrich,‚Äù feature learning, <span class="math inline">\(\mu\)</span>P dynamics led to a paradigm shift in deep learning theory. Most later work uses or relates to it in some way. It‚Äôs thus very important to understand. Any deep learning theorist should be able to sit down and derive the <span class="math inline">\(\mu\)</span>P parameterization, or something equivalent to it, from first principles. It‚Äôs difficult to do relevant work in 2025 without it!</p>
      <p>It‚Äôs worth noting that, unlike the NTK limit, the <span class="math inline">\(\mu\)</span>P limit is very difficult to study analytically. In the NTK limit, we have kernel behavior, simple gradient descent dynamics, a convex loss surface (using squared loss), and lots of older theoretical tools that we can bring to bear. In the <span class="math inline">\(\mu\)</span>P limit, we have none of this. To our knowledge, nobody‚Äôs even shown a general result that a deep network in the <span class="math inline">\(\mu\)</span>P limit <em>converges,</em> let alone characterized the solution that‚Äôs found!</p>
      <div class="question-box">
      <p><strong>Open question:</strong> when does a network in the <span class="math inline">\(\mu\)</span>P limit converge under gradient descent?</p>
      </div>
      <p>In the NTK limit, we can study the model with the well-established math of kernel theory (which has been developed further expressly for the study of the NTK). In the <span class="math inline">\(\mu\)</span>P limit, the best we have so far are rather complex calculational frameworks:</p>
      <ul>
      <li>The dynamical mean-field theory (DMFT) frameworks of <a href="https://arxiv.org/abs/2205.09653">[Bordelon and Pehlevan (2022)]</a> and <a href="https://arxiv.org/abs/2402.03220">[Dandi et al.¬†(2024)]</a> let one compute feature distributions of infinite-width networks in the rich regime, but it is quite complicated, and it is difficult to extract analytical insight. (It is nonetheless useful for scaling calculations.) The Tensor Programs framework of <a href="https://proceedings.mlr.press/v139/yang21c.html">[Yang and Hu (2021)]</a> allows one to perform the same calculations with random matrix theory language.
      <ul>
      <li>These are specialized tools and are not essential on a first pass through deep learning theory.</li>
      </ul></li>
      </ul>
      <div class="question-box">
      <p><strong>Open question:</strong> is there a simple calculational framework ‚Äî potentially making realistic simplifying assumptions ‚Äî that allows us to quantitatively study feature evolution in the rich regime?</p>
      </div>
      <h3 id="onwards-towards-infinite-depth">Onwards: towards infinite depth</h3>
      <p>Early in this chapter, we took width to infinity, which allowed us a host of useful calculational tools. We can also take depth to infinity. There are several ways to do this, but the upshot is that one quickly encounters stability problems, so a ResNet formulation in which each layer gets a small premultiplier seems like the most promising choice.</p>
      <ul>
      <li><a href="https://arxiv.org/abs/2309.16620">[Bordelon et al.¬†(2023)]</a> and <a href="https://arxiv.org/abs/2310.02244">[Yang et al.¬†(2023)]</a> give treatments of <span class="math inline">\(\mu\)</span>P at large depth. They conclude with slightly different scaling recommendations for layerwise premultipliers.
      <ul>
      <li>This is probably useful to understand for the future but is not yet essential knowledge. As with <span class="math inline">\(\mu\)</span>P, the most important thing to get out of either paper is the scaling calculation that gives the hyperparameter prescription.</li>
      </ul></li>
      </ul>
      <div class="question-box">
      <p><strong>Open question:</strong> is there a simple calculational framework for studying the feature evolution of an infinite-depth network?</p>
      </div>
      <section class="footnotes" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1" role="doc-endnote"><p>‚Ä¶or feeding very large or small values to a norm layer, or representing them in finite precision and losing bits, or some other malady.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </main>
  </article>

  <hr><div class="sequence-toc"><h3>A Quickstart Guide to Learning Mechanics</h3><ol><li><a href="../quickstart/introduction.html">Introduction: what do you want to understand?</a></li><li><strong>...the average size of hidden representations?</strong></li><li><a href="../quickstart/hyperparameter-selection.html">üöß ...hyperparameter selection (and why should theorists care)?</a></li><li><a href="../quickstart/optimization.html">üöß ...the convergence and stability of optimization?</a></li><li><a href="../quickstart/feature-learning.html">üöß ...feature learning and the final network weights?</a></li><li><a href="../quickstart/generalization.html">üöß ...generalization?</a></li><li><a href="../quickstart/sparsity.html">üöß ...neuron-level sparsity?</a></li><li><a href="../quickstart/data-structure.html">üöß ...the structure in the data?</a></li><li><a href="../quickstart/conclusion.html">üöß Places to make a difference</a></li></ol></div><div class="back-to-top"><a href="#top"><i class="fas fa-arrow-circle-up"></i></a></div>

  <div class="comments">
    <h2>Comments</h2>
    <script src="https://giscus.app/client.js"
            data-repo="analyticinterp/analyticinterp.github.io"
            data-repo-id="R_kgDONkWUAw"
            data-category="General"
            data-category-id=""
            data-mapping="pathname"
            data-theme="light"
            data-lang="en"
            crossorigin="anonymous"
            async>
    </script>
  </div>


  <script>
    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      
      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
      
      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = newTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';
      
      // Update Giscus theme
      const giscusFrame = document.querySelector('iframe.giscus-frame');
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage({
          giscus: { setConfig: { theme: newTheme } }
        }, 'https://giscus.app');
      }
    }
    
    // Initialize theme on page load
    document.addEventListener('DOMContentLoaded', function() {
      const savedTheme = localStorage.getItem('theme') || 'light';
      document.documentElement.setAttribute('data-theme', savedTheme);
      
      // Update button icon
      const icon = document.querySelector('.theme-toggle i');
      icon.className = savedTheme === 'dark' ? 'fas fa-sun' : 'fas fa-moon';
      
    });
  </script>
</body>
</html> 