---
title: "Want to understand neuron-level sparsity?"
toc_title: "üöß ...neuron-level sparsity?"
author: "[long list of authors]"
date: "2025-09-01"
description: "Mathematical analysis of sparsity patterns in neural network activations and weights."
sequence: "quickstart"
sequence_title: "A Quickstart Guide to Learning Mechanics"
sequence_description: "A comprehensive guide to understanding the mathematical foundations of deep learning, from optimization to generalization."
sequence_order: 7
---

[TO BE WRITTEN]


- subject of a lot of mechinterp, rather less mathematical theory
- studies of ICA (as opposed to PCA)
- L1-norm over L2-norm
- some of Arthur‚Äôs work (deep nets induce some kind of L1-ish sparsity)
    - esp from small init!
- ‚Äúcondensation phenomenon‚Äù
- some of Dan Kunin‚Äôs recent work (AGF)
- Q: can you study neuron densities and avoid worrying about neuron sparsity?
- Q: do inf-width muP nets learn sparsity? (probably, right? so does that mean Anthropic‚Äôs superposition hyp is wrong?)