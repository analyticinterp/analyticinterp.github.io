<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Analytic Interpretability</title>
  
  <!-- Our minimal styles -->
  <link rel="stylesheet" href="static/style.css">
  
  <meta name="description" content="Understanding deep learning through theory">
</head>
<body>
  <header>
    <h1>Analytic Interpretability</h1>
    <p>Notes on deep learning theory</p>
  </header>

  <main>
    <div class="post-list">
            <div class="date-header">January 2025</div>
      <a href="understanding-attention.html" class="post-entry">
        <h3>Understanding Attention Mechanisms</h3>
        <div class="post-byline">Eric Doe, January 20, 2025</div>
        <div class="post-description">A mathematical exploration of attention mechanisms in transformers</div>
      </a>
      <a href="gradient-dynamics.html" class="post-entry">
        <h3>Gradient Dynamics in Deep Networks</h3>
        <div class="post-byline">Jane Smith, January 18, 2025</div>
        <div class="post-description">Analyzing the flow of gradients through deep neural architectures</div>
      </a>
      <a href="transformer-universality.html" class="post-entry">
        <h3>On the Universality of Transformer Representations</h3>
        <div class="post-byline">No one, January 15, 2025</div>
        <div class="post-description">We prove that transformer networks with sufficient width and depth can approximate any continuous sequence-to-sequence function, extending classical universality results to the attention mechanism.</div>
      </a>
    </div>
    
    <hr>
    
    <section class="about-section">
      <h2>About</h2>
      <!-- <p>The project of understanding deep learning will be to the 21st century what physics was to the 20th.<p> -->
      <p>The project of understanding deep learning will be <i>the</i> intellectual project of the early 21st century, like physics was to the early 20th.</p>
      
      <p>Deep learning has achieved wonders. And we don't know why. While tens of billions will be spent this year on training massive neural networks, this investment is a leap of faith. It is motivated by extrapolation, rather than by theoretical understanding. We do not yet have a unified framework for thinking about what it is that neural networks learn and why they work well.</p>

      <p>And yet, a story is slowly emerging. It is a story about the structure of data. It is a story about optimization, and about scaling. And it is ultimately a story about intelligence: about what minds can exist in this world, and how they are formed.</p>

      <p>Most of us working on this story feel as if we are stuck in a maze. We wander around. We map out parts of the maze we know don't contain the exit. Some of us may have good ideas for directions to go next. We've come together here to develop a map of the parts we've explored, the parts we want to explore next.</p>

      <p>We are a group of young scientists trying to understand deep learning.</p>

      <ul>
        <li><a href="https://abatanasov.com/">Alex Atanasov</a></li>
        <li><a href="https://jeremybernste.in/">Jeremy Bernstein</a></li>
        <li><a href="https://blakebordelon.github.io/">Blake Bordelon</a></li>
        <li><a href="https://jmcohen.github.io/">Jeremy Cohen</a></li>
        <li><a href="https://web.math.princeton.edu/~ad27/">Alex Damian</a></li>
        <li><a href="https://nikhil-ghosh-berkeley.github.io/">Nikhil Ghosh</a></li>
        <li><a href="https://florentinguth.github.io/">Florentin Guth</a></li>
        <li><a href="https://sites.google.com/view/arthurjacot">Arthur Jacot</a></li>
        <li><a href="https://dkarkada.xyz/">Dhruva Karkada</a></li>
        <li><a href="https://daniel-kunin.com/">Daniel Kunin</a></li>
        <li><a href="https://alexandrumeterez.github.io/">Alex Meterez</a></li>
        <li><a href="https://ericjmichaud.com/">Eric Michaud</a></li>
        <li><a href="https://misiakie.github.io/">Theodor Misiakiewicz</a></li>
        <li><a href="https://berkan.xyz/">Berkan Ottlik</a></li>
        <li><a href="https://aditradha.com/">Adit Radha</a></li>
        <li><a href="https://james-simon.github.io/">Jamie Simon</a></li>
        <li><a href="https://www.linkedin.com/in/joey-turnbull/">Joey Turnbull</a></li>
        <li><a href="https://jzv.io/">Jacob Zavatone-Veth</a></li>
      </ul>
    </section>
    
    <footer>
      <p><a href="feed.xml">RSS Feed</a></p>
    </footer>
  </main>
</body>
</html> 