<?xml version="1.0" ?>
<rss version="2.0">
  <channel>
    <title>Analytic Interpretability</title>
    <link>https://analyticinterp.github.io</link>
    <description>Understanding deep learning through theory</description>
    <language>en-us</language>
    <item>
      <title>Understanding Attention Mechanisms</title>
      <link>https://analyticinterp.github.io/understanding-attention.html</link>
      <guid>https://analyticinterp.github.io/understanding-attention.html</guid>
      <description>A mathematical exploration of attention mechanisms in transformers</description>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Gradient Dynamics in Deep Networks</title>
      <link>https://analyticinterp.github.io/gradient-dynamics.html</link>
      <guid>https://analyticinterp.github.io/gradient-dynamics.html</guid>
      <description>Analyzing the flow of gradients through deep neural architectures</description>
      <pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>On the Universality of Transformer Representations</title>
      <link>https://analyticinterp.github.io/transformer-universality.html</link>
      <guid>https://analyticinterp.github.io/transformer-universality.html</guid>
      <description>We prove that transformer networks with sufficient width and depth can approximate any continuous sequence-to-sequence function, extending classical universality results to the attention mechanism.</description>
      <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
